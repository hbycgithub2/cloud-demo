# 大厂 SQL 优化经典案例 - 超详细版

> **说明**：本文档整理了大厂公开分享的 SQL 优化经典案例和通用解决方案，用于学习和参考。具体技术细节基于公开资料和行业最佳实践整理。

## 目录

1. [电商大促：订单查询优化](#案例-1电商大促订单查询优化)
2. [外卖平台：位置数据更新优化](#案例-2外卖平台位置数据更新优化)
3. [短视频平台：推荐系统优化](#案例-3短视频平台推荐系统优化)
4. [社交平台：深分页优化](#案例-4社交平台深分页优化)
5. [电商平台：搜索优化](#案例-5电商平台搜索优化)
6. [出行平台：高并发写入优化](#案例-6出行平台高并发写入优化)

---

## 案例 1：电商大促 - 订单查询优化

### 背景

**场景**：电商大促期间（如双 11、618）订单量暴增导致查询变慢

**典型问题**：
- 订单查询接口响应时间从 0.1 秒飙升到 10+ 秒
- 用户无法查看订单，投诉量暴增
- 数据库 CPU 飙升到 90%+
- 数据量：订单表达到亿级甚至十亿级

**技术挑战**：
- 短时间内订单量暴增（平时 10 倍以上）
- 用户查询订单频率高
- 历史订单和新订单混在一起

---

### 问题发现

```
┌─────────────────────────────────────────────────────────────┐
│                    问题发现过程                                │
└─────────────────────────────────────────────────────────────┘

凌晨 00:00：双 11 开始
  ↓
  订单量暴增：每秒 50 万笔订单

凌晨 00:30：监控告警
  ↓
  告警信息：
    - 订单查询接口响应时间：30 秒
    - 数据库 CPU：95%
    - 慢查询数量：10000 条/分钟

凌晨 00:35：用户投诉
  ↓
  投诉内容：
    - "订单查不到了！"
    - "页面一直转圈圈！"
    - "我的钱扣了，订单在哪？"

凌晨 00:40：紧急会议
  ↓
  参与人员：
    - DBA（数据库管理员）
    - 后端开发
    - 运维
    - 产品经理

凌晨 00:45：开始排查
  ↓
  排查方向：
    1. 数据库慢查询
    2. 服务器资源
    3. 网络问题
```

---

### 问题定位

```
┌─────────────────────────────────────────────────────────────┐
│                    问题定位过程                                │
└─────────────────────────────────────────────────────────────┘

步骤 1：查看慢查询日志
  ↓
  DBA 查看慢查询日志：
    tail -f /var/log/mysql/slow.log
  
  发现慢查询 SQL：
    SELECT * FROM orders 
    WHERE user_id = 123456 
      AND create_time > '2023-11-11 00:00:00'
    ORDER BY create_time DESC
    LIMIT 20;
  
  执行时间：30 秒
  执行次数：每秒 1000 次

步骤 2：分析执行计划
  ↓
  EXPLAIN SELECT * FROM orders 
  WHERE user_id = 123456 
    AND create_time > '2023-11-11 00:00:00'
  ORDER BY create_time DESC
  LIMIT 20;
  
  结果：
    type: ALL（全表扫描）
    key: NULL（没有使用索引）
    rows: 1000000000（扫描 10 亿行）
    Extra: Using where; Using filesort
  
  问题确认：
    ❌ 全表扫描
    ❌ 没有使用索引
    ❌ 需要排序

步骤 3：检查索引
  ↓
  SHOW INDEX FROM orders;
  
  结果：
    只有主键索引（id）
    没有 user_id 和 create_time 的索引
  
  问题根源：
    ❌ 缺少索引
    ❌ 双 11 订单量暴增，全表扫描导致查询慢

步骤 4：评估影响
  ↓
  影响范围：
    - 受影响用户：1000 万
    - 订单查询失败率：80%
    - 预计损失：5000 万元
  
  紧急程度：P0（最高优先级）
```

---

### 解决方案

```
┌─────────────────────────────────────────────────────────────┐
│                    解决方案执行过程                            │
└─────────────────────────────────────────────────────────────┘

方案 1：紧急添加索引（立即执行）
  ↓
  凌晨 01:00：DBA 添加索引
    CREATE INDEX idx_user_time 
    ON orders(user_id, create_time);
  
  问题：
    ❌ 订单表有 10 亿条数据
    ❌ 添加索引需要 2 小时
    ❌ 期间会锁表，影响写入
  
  决策：
    ⚠️ 不能直接添加索引（会锁表）
    ✅ 需要其他方案

方案 2：读写分离 + 从库添加索引（立即执行）
  ↓
  凌晨 01:05：执行方案
  
  步骤 1：在从库添加索引
    从库 1：CREATE INDEX idx_user_time ON orders(user_id, create_time);
    从库 2：CREATE INDEX idx_user_time ON orders(user_id, create_time);
    从库 3：CREATE INDEX idx_user_time ON orders(user_id, create_time);
    
    耗时：2 小时（不影响主库写入）
  
  步骤 2：查询流量切换到从库
    凌晨 03:00：索引添加完成
    凌晨 03:05：查询流量切换到从库
  
  步骤 3：验证效果
    查询响应时间：从 30 秒降到 0.1 秒
    数据库 CPU：从 95% 降到 30%
    用户投诉：停止
  
  结果：
    ✅ 问题解决
    ✅ 用户可以正常查询订单

方案 3：主库添加索引（后续执行）
  ↓
  11 月 12 日凌晨 02:00：流量低峰期
  
  在主库添加索引：
    CREATE INDEX idx_user_time 
    ON orders(user_id, create_time);
  
  耗时：2 小时
  
  结果：
    ✅ 主库也有索引
    ✅ 读写都快

方案 4：长期优化（后续执行）
  ↓
  11 月 15 日：
  
  1. 分库分表
     原表：orders（10 亿条数据）
     拆分后：
       orders_2023_11（1 亿条数据）
       orders_2023_10（1 亿条数据）
       ...
  
  2. 冷热数据分离
     热数据：最近 3 个月的订单（查询频繁）
     冷数据：3 个月前的订单（查询少）
     
     热数据存储：MySQL
     冷数据存储：HBase（大数据存储）
  
  3. 添加缓存
     Redis 缓存最近 7 天的订单
     缓存命中率：90%
  
  结果：
    ✅ 查询速度更快
    ✅ 数据库压力更小
```

---

### 完整流程图

```
┌─────────────────────────────────────────────────────────────┐
│            电商大促订单查询慢 - 完整处理流程                    │
└─────────────────────────────────────────────────────────────┘

大促开始
  ↓
  订单量暴增：平时的 10-50 倍

监控告警
  ↓
  订单查询接口响应时间：10+ 秒
  数据库 CPU：90%+
  慢查询数量激增

用户投诉
  ↓
  "订单查不到了！"
  "页面一直转圈圈！"

紧急会议
  ↓
  DBA + 后端 + 运维 + 产品

开始排查
  ↓
  查看慢查询日志
  分析执行计划（EXPLAIN）
  检查索引

问题定位
  ↓
  问题：缺少索引，全表扫描
  影响：大量用户，业务损失严重

执行紧急方案
  ↓
  方案：读写分离 + 从库添加索引
  
  步骤 1：在从库添加索引（不影响主库写入）
  步骤 2：查询流量切换到从库

验证效果
  ↓
  查询响应时间：恢复正常
  数据库 CPU：降低
  用户投诉：停止

问题解决
  ↓
  ✅ 用户可以正常查询订单

后续优化
  ↓
  1. 主库添加索引（流量低峰期）
  2. 分库分表（长期方案）
  3. 冷热数据分离
  4. 添加缓存（Redis）

────────────────────────────────────────────────────────────

关键决策：
  1. 不能直接在主库添加索引（会锁表，影响写入）
  2. 先在从库添加索引，再切换流量（风险小）
  3. 等流量低峰期再在主库添加索引
  4. 长期优化：分库分表 + 缓存

经验教训：
  1. 提前做好容量规划和压测
  2. 提前添加必要的索引
  3. 做好监控和告警
  4. 准备应急预案（读写分离、降级、限流）
  5. 大促前进行全链路压测
```

---

## 案例 2：外卖平台 - 位置数据更新优化

### 背景

**场景**：外卖/出行平台的实时位置更新

**典型问题**：
- 位置更新延迟严重（5-10 秒）
- 用户看不到骑手/司机实时位置
- 数据库写入 TPS 瓶颈
- 磁盘 IO 达到 100%

**技术挑战**：
- 高频写入（每秒数十万到百万次）
- 历史位置数据量巨大（百亿级）
- 需要实时查询最新位置

---

### 问题发现

```
┌─────────────────────────────────────────────────────────────┐
│                    问题发现过程                                │
└─────────────────────────────────────────────────────────────┘

上午 11:00：午餐高峰期
  ↓
  骑手数量：100 万
  位置更新频率：每秒 1 次
  每秒更新次数：100 万次

上午 11:30：监控告警
  ↓
  告警信息：
    - 位置更新接口响应时间：10 秒
    - 数据库写入 TPS：从 100 万降到 10 万
    - 数据库磁盘 IO：100%

上午 11:35：用户投诉
  ↓
  投诉内容：
    - "骑手位置不动了！"
    - "骑手到哪了？看不到！"
    - "外卖到底还有多久？"

上午 11:40：紧急排查
```

---

### 问题定位

```
┌─────────────────────────────────────────────────────────────┐
│                    问题定位过程                                │
└─────────────────────────────────────────────────────────────┘

步骤 1：查看慢查询日志
  ↓
  发现慢查询 SQL：
    UPDATE rider_location 
    SET lat = 39.9042, lng = 116.4074, update_time = NOW()
    WHERE rider_id = 123456;
  
  执行时间：10 秒
  执行次数：每秒 100 万次

步骤 2：分析执行计划
  ↓
  EXPLAIN UPDATE rider_location 
  SET lat = 39.9042, lng = 116.4074, update_time = NOW()
  WHERE rider_id = 123456;
  
  结果：
    type: ref（使用索引）
    key: idx_rider_id（使用了索引）
    rows: 1（只更新 1 行）
  
  疑问：
    ✅ 使用了索引
    ✅ 只更新 1 行
    ❓ 为什么还慢？

步骤 3：检查表结构
  ↓
  SHOW CREATE TABLE rider_location;
  
  结果：
    CREATE TABLE rider_location (
      id BIGINT PRIMARY KEY,
      rider_id BIGINT,
      lat DECIMAL(10, 6),
      lng DECIMAL(10, 6),
      update_time DATETIME,
      create_time DATETIME,
      -- 还有 50 个其他字段（历史位置、速度、方向等）
      INDEX idx_rider_id (rider_id)
    );
  
  问题发现：
    ❌ 表有 100 亿条数据（历史位置都存在这张表）
    ❌ 表有 50 多个字段（字段太多）
    ❌ 每次更新都要写入磁盘（磁盘 IO 瓶颈）

步骤 4：问题根源
  ↓
  问题 1：表太大（100 亿条数据）
    - 历史位置和当前位置混在一起
    - 查询和更新都慢
  
  问题 2：字段太多（50 多个字段）
    - 每次更新都要写入 50 多个字段
    - 磁盘 IO 高
  
  问题 3：更新频率太高（每秒 100 万次）
    - 数据库写入压力大
    - 磁盘 IO 瓶颈
```

---

### 解决方案

```
┌─────────────────────────────────────────────────────────────┐
│                    解决方案执行过程                            │
└─────────────────────────────────────────────────────────────┘

方案 1：紧急方案 - 使用 Redis 缓存（立即执行）
  ↓
  上午 11:45：执行方案
  
  步骤 1：骑手位置写入 Redis
    // 骑手位置更新
    redis.set("rider:location:" + riderId, {
      lat: 39.9042,
      lng: 116.4074,
      updateTime: System.currentTimeMillis()
    }, 60);  // 过期时间 60 秒
  
  步骤 2：异步写入 MySQL
    // 每 10 秒批量写入 MySQL
    定时任务：
      1. 从 Redis 读取所有骑手位置
      2. 批量写入 MySQL
      3. 每次写入 1000 条
  
  步骤 3：查询从 Redis 读取
    // 查询骑手位置
    location = redis.get("rider:location:" + riderId);
    if (location == null) {
      location = mysql.query("SELECT * FROM rider_location WHERE rider_id = ?", riderId);
    }
  
  结果：
    ✅ 位置更新响应时间：从 10 秒降到 0.01 秒
    ✅ 数据库写入压力：从 100 万/秒降到 10 万/秒
    ✅ 用户可以实时看到骑手位置

方案 2：长期方案 - 表结构优化（后续执行）
  ↓
  8 月 15 日：执行优化
  
  优化 1：拆分表
    原表：rider_location（100 亿条数据，50 个字段）
    
    拆分后：
      rider_location_current（当前位置，100 万条数据，5 个字段）
        - id
        - rider_id
        - lat
        - lng
        - update_time
      
      rider_location_history（历史位置，100 亿条数据，50 个字段）
        - 按天分表：rider_location_history_20230815
  
  优化 2：使用内存表
    CREATE TABLE rider_location_current (
      rider_id BIGINT PRIMARY KEY,
      lat DECIMAL(10, 6),
      lng DECIMAL(10, 6),
      update_time BIGINT
    ) ENGINE=MEMORY;  -- 使用内存表
  
  优化 3：定时归档
    每天凌晨 2 点：
      1. 将当天的位置数据归档到历史表
      2. 清空当前位置表
  
  结果：
    ✅ 当前位置表只有 100 万条数据
    ✅ 字段只有 5 个
    ✅ 使用内存表，速度更快
    ✅ 历史数据归档，不影响当前查询

方案 3：架构优化（后续执行）
  ↓
  9 月 1 日：架构升级
  
  优化 1：使用 MongoDB 存储位置数据
    MongoDB 特点：
      - 适合存储位置数据
      - 写入性能高
      - 支持地理位置查询
  
  优化 2：使用 Kafka 消息队列
    骑手位置更新流程：
      1. 骑手 APP → Kafka（位置消息）
      2. Kafka → Redis（实时位置）
      3. Kafka → MongoDB（持久化）
      4. Kafka → 大数据平台（分析）
  
  优化 3：使用 CDN 加速
    用户查询骑手位置：
      1. 用户 → CDN（缓存）
      2. CDN → Redis（实时位置）
  
  结果：
    ✅ 写入性能：从 10 万/秒提升到 100 万/秒
    ✅ 查询性能：从 0.1 秒降到 0.01 秒
    ✅ 系统稳定性：更高
```

---

### 完整流程图

```
┌─────────────────────────────────────────────────────────────┐
│          外卖平台位置数据更新慢 - 完整处理流程                  │
└─────────────────────────────────────────────────────────────┘

用餐高峰期
  ↓
  骑手数量：数十万
  位置更新频率：每秒 1 次

监控告警
  ↓
  位置更新接口响应时间：5-10 秒
  数据库磁盘 IO：100%

用户投诉
  ↓
  "骑手位置不动了！"
  "外卖到底还有多久？"

紧急排查
  ↓
  查看慢查询日志
  分析执行计划
  检查表结构

问题定位
  ↓
  问题 1：表太大（历史位置和当前位置混在一起）
  问题 2：字段太多（50+ 个字段）
  问题 3：更新频率太高（每秒数十万次）

执行紧急方案
  ↓
  方案：使用 Redis 缓存
  
  步骤 1：骑手位置写入 Redis（实时）
  步骤 2：异步批量写入 MySQL（每 10 秒）
  步骤 3：查询从 Redis 读取

验证效果
  ↓
  位置更新响应时间：0.01 秒
  数据库写入压力：降低 90%
  用户可以实时看到骑手位置

问题解决
  ↓
  ✅ 用户体验恢复正常

长期优化
  ↓
  1. 拆分表（当前位置 + 历史位置）
  2. 使用内存表（当前位置）
  3. 定时归档历史数据

架构升级
  ↓
  1. 使用 MongoDB 存储位置数据（更适合）
  2. 使用 Kafka 消息队列（解耦）
  3. 使用 CDN 加速查询

────────────────────────────────────────────────────────────

关键决策：
  1. 紧急方案：使用 Redis 缓存（立即生效）
  2. 长期方案：拆分表 + 内存表（彻底解决）
  3. 架构升级：MongoDB + Kafka（性能更好）

经验教训：
  1. 不要把历史数据和当前数据混在一起
  2. 高频更新的数据用 Redis 缓存
  3. 定时归档历史数据
  4. 选择合适的存储方案（MongoDB 更适合位置数据）
```

---

## 案例 3：短视频平台 - 推荐系统优化

### 背景

**场景**：短视频/内容平台的个性化推荐

**典型问题**：
- 推荐接口响应时间长（3-5 秒）
- 用户刷视频卡顿
- 数据库 CPU 持续高负载
- 内容数据量巨大（数十亿级）

**技术挑战**：
- 需要过滤用户已看过的内容
- 需要复杂的排序逻辑
- 需要关联多张表（用户、内容、互动）

---

### 问题发现

```
┌─────────────────────────────────────────────────────────────┐
│                    问题发现过程                                │
└─────────────────────────────────────────────────────────────┘

晚上 20:00：晚间高峰期
  ↓
  在线用户：5000 万
  视频推荐请求：每秒 100 万次

晚上 20:30：监控告警
  ↓
  告警信息：
    - 视频推荐接口响应时间：5 秒
    - 数据库 CPU：90%
    - 慢查询数量：50000 条/分钟

晚上 20:35：用户反馈
  ↓
  用户反馈：
    - "刷视频卡顿！"
    - "视频加载慢！"
    - "一直转圈圈！"

晚上 20:40：紧急排查
```

---

### 问题定位

```
┌─────────────────────────────────────────────────────────────┐
│                    问题定位过程                                │
└─────────────────────────────────────────────────────────────┘

步骤 1：查看慢查询日志
  ↓
  发现慢查询 SQL：
    SELECT v.*, u.nickname, u.avatar 
    FROM videos v
    LEFT JOIN users u ON v.user_id = u.id
    WHERE v.status = 1
      AND v.create_time > DATE_SUB(NOW(), INTERVAL 7 DAY)
      AND v.id NOT IN (
        SELECT video_id FROM user_watch_history 
        WHERE user_id = 123456
      )
    ORDER BY v.like_count DESC, v.create_time DESC
    LIMIT 20;
  
  执行时间：5 秒
  执行次数：每秒 100 万次

步骤 2：分析执行计划
  ↓
  EXPLAIN ...
  
  结果：
    主查询：
      type: range
      key: idx_status_time
      rows: 10000000（扫描 1000 万行）
    
    子查询：
      type: ref
      key: idx_user_id
      rows: 5000（扫描 5000 行）
    
    问题：
      ❌ 子查询执行了 100 万次（每个用户都执行一次）
      ❌ NOT IN 性能差
      ❌ 需要排序（ORDER BY）

步骤 3：问题根源
  ↓
  问题 1：子查询性能差
    - NOT IN 子查询执行了 100 万次
    - 每次子查询扫描 5000 行
    - 总扫描行数：100 万 × 5000 = 50 亿行
  
  问题 2：排序性能差
    - ORDER BY like_count DESC, create_time DESC
    - 需要对 1000 万行数据排序
    - 排序耗时长
  
  问题 3：JOIN 性能差
    - LEFT JOIN users 表
    - 每次都要关联用户表
```

---

### 解决方案

```
┌─────────────────────────────────────────────────────────────┐
│                    解决方案执行过程                            │
└─────────────────────────────────────────────────────────────┘

方案 1：紧急方案 - 优化 SQL（立即执行）
  ↓
  晚上 20:45：执行方案
  
  优化 1：去掉子查询，改用 LEFT JOIN
    -- 原始 SQL（慢）
    SELECT v.*, u.nickname, u.avatar 
    FROM videos v
    LEFT JOIN users u ON v.user_id = u.id
    WHERE v.status = 1
      AND v.create_time > DATE_SUB(NOW(), INTERVAL 7 DAY)
      AND v.id NOT IN (
        SELECT video_id FROM user_watch_history 
        WHERE user_id = 123456
      )
    ORDER BY v.like_count DESC, v.create_time DESC
    LIMIT 20;
    
    -- 优化后的 SQL（快）
    SELECT v.*, u.nickname, u.avatar 
    FROM videos v
    LEFT JOIN users u ON v.user_id = u.id
    LEFT JOIN user_watch_history h 
      ON v.id = h.video_id AND h.user_id = 123456
    WHERE v.status = 1
      AND v.create_time > DATE_SUB(NOW(), INTERVAL 7 DAY)
      AND h.video_id IS NULL  -- 没有观看过的视频
    ORDER BY v.like_count DESC, v.create_time DESC
    LIMIT 20;
  
  优化 2：添加索引
    CREATE INDEX idx_user_video 
    ON user_watch_history(user_id, video_id);
  
  结果：
    ✅ 响应时间：从 5 秒降到 0.5 秒
    ✅ 扫描行数：从 50 亿行降到 1000 万行
    ✅ 性能提升：10 倍

方案 2：长期方案 - 使用推荐系统（后续执行）
  ↓
  10 月 15 日：上线推荐系统
  
  架构：
    1. 离线计算（每小时）
       - 使用大数据平台（Spark）
       - 计算每个用户的推荐视频列表
       - 存储到 Redis
    
    2. 实时推荐
       - 用户刷视频 → 从 Redis 读取推荐列表
       - Redis 没有 → 调用推荐算法
       - 推荐算法 → 返回视频列表
    
    3. 实时更新
       - 用户观看视频 → 更新推荐模型
       - 用户点赞视频 → 更新推荐模型
  
  流程：
    用户刷视频
      ↓
    从 Redis 读取推荐列表
      key = "recommend:user:" + userId
      value = [video1, video2, video3, ...]
      ↓
    如果 Redis 没有
      ↓
    调用推荐算法
      - 协同过滤
      - 内容推荐
      - 热门推荐
      ↓
    返回视频列表
      ↓
    写入 Redis（过期时间 1 小时）
  
  结果：
    ✅ 响应时间：从 0.5 秒降到 0.01 秒
    ✅ 推荐准确率：提升 30%
    ✅ 用户观看时长：提升 50%

方案 3：架构优化（后续执行）
  ↓
  11 月 1 日：架构升级
  
  优化 1：使用 Elasticsearch 存储视频数据
    Elasticsearch 特点：
      - 适合全文搜索
      - 支持复杂查询
      - 性能高
    
    数据同步：
      MySQL → Binlog → Canal → Kafka → Elasticsearch
  
  优化 2：使用 CDN 缓存视频信息
    视频信息缓存到 CDN
    用户查询 → CDN → Redis → Elasticsearch
  
  优化 3：使用分布式缓存
    Redis 集群：
      - 主从复制
      - 哨兵模式
      - 集群模式
  
  结果：
    ✅ 响应时间：0.01 秒
    ✅ 系统稳定性：更高
    ✅ 支持更复杂的推荐算法
```

---

## 案例 4：社交平台 - 深分页优化

### 背景

**场景**：社交平台的动态/朋友圈/微博分页查询

**典型问题**：
- 翻页越多越慢（第 100 页需要 10+ 秒）
- 用户体验差
- 数据库扫描大量数据后丢弃

**技术挑战**：
- 数据量巨大（千亿级）
- 需要按时间倒序排列
- 用户可能翻很多页

---

### 问题定位

```
┌─────────────────────────────────────────────────────────────┐
│                    问题定位过程                                │
└─────────────────────────────────────────────────────────────┘

步骤 1：查看慢查询日志
  ↓
  发现慢查询 SQL：
    SELECT * FROM moments 
    WHERE user_id IN (
      SELECT friend_id FROM friends WHERE user_id = 123456
    )
    ORDER BY create_time DESC
    LIMIT 1000, 10;  -- 第 100 页
  
  执行时间：10 秒

步骤 2：分析执行计划
  ↓
  EXPLAIN ...
  
  结果：
    type: range
    key: idx_user_time
    rows: 100000000（扫描 1 亿行）
    Extra: Using filesort
  
  问题：
    ❌ 深分页问题（LIMIT 1000, 10）
    ❌ 需要扫描 1010 行，丢弃前 1000 行
    ❌ 需要排序

步骤 3：问题根源
  ↓
  问题：深分页
    - 用户有 500 个好友
    - 每个好友平均 1000 条朋友圈
    - 总共 50 万条朋友圈
    - 翻到第 100 页，需要扫描 1010 条
    - 但实际扫描了 1 亿行（因为需要排序）
```

---

### 解决方案

```
┌─────────────────────────────────────────────────────────────┐
│                    解决方案执行过程                            │
└─────────────────────────────────────────────────────────────┘

方案 1：使用游标分页（推荐）
  ↓
  原理：
    记录上次查询的最后一条数据的 id
    下次查询从这个 id 开始
  
  实现：
    -- 第 1 页
    SELECT * FROM moments 
    WHERE user_id IN (...)
    ORDER BY create_time DESC, id DESC
    LIMIT 10;
    
    -- 返回结果：
    最后一条数据：id = 1000, create_time = '2023-09-01 12:00:00'
    
    -- 第 2 页
    SELECT * FROM moments 
    WHERE user_id IN (...)
      AND (create_time < '2023-09-01 12:00:00' 
           OR (create_time = '2023-09-01 12:00:00' AND id < 1000))
    ORDER BY create_time DESC, id DESC
    LIMIT 10;
  
  优点：
    ✅ 不需要扫描前面的数据
    ✅ 性能稳定（不管翻到第几页）
    ✅ 响应时间：0.1 秒
  
  缺点：
    ⚠️ 不能跳页（只能上一页、下一页）
    ⚠️ 适合移动端（移动端一般不跳页）

方案 2：使用 Redis 缓存朋友圈列表
  ↓
  原理：
    把用户的朋友圈列表缓存到 Redis
    使用 Redis 的 ZSET（有序集合）
  
  实现：
    // 发布朋友圈
    redis.zadd("moments:user:" + userId, timestamp, momentId);
    
    // 推送给好友
    List<Long> friendIds = getFriendIds(userId);
    for (Long friendId : friendIds) {
      redis.zadd("moments:timeline:" + friendId, timestamp, momentId);
    }
    
    // 查询朋友圈（第 1 页）
    List<Long> momentIds = redis.zrevrange("moments:timeline:" + userId, 0, 9);
    
    // 查询朋友圈（第 2 页）
    List<Long> momentIds = redis.zrevrange("moments:timeline:" + userId, 10, 19);
  
  优点：
    ✅ 速度快（Redis 内存存储）
    ✅ 支持分页
    ✅ 响应时间：0.01 秒
  
  缺点：
    ⚠️ 占用内存（需要缓存所有朋友圈）
    ⚠️ 需要实时推送（发布朋友圈时推送给所有好友）

方案 3：混合方案（推荐）
  ↓
  原理：
    前 10 页：使用 Redis 缓存
    10 页以后：使用游标分页
  
  实现：
    if (page <= 10) {
      // 从 Redis 读取
      List<Long> momentIds = redis.zrevrange("moments:timeline:" + userId, 
        (page - 1) * 10, page * 10 - 1);
    } else {
      // 使用游标分页
      List<Moment> moments = momentMapper.selectByLastId(lastId, 10);
    }
  
  优点：
    ✅ 前 10 页速度快（Redis）
    ✅ 10 页以后也不慢（游标分页）
    ✅ 节省内存（只缓存前 10 页）
```

---

## 案例 5：电商平台 - 搜索优化

### 背景

**场景**：电商平台的商品搜索

**典型问题**：
- 搜索响应时间长（2-3 秒）
- 模糊搜索无法使用索引
- 搜索结果不准确
- 用户体验差，转化率下降

**技术挑战**：
- 商品数据量巨大（亿级）
- 需要支持模糊搜索、拼音搜索
- 需要多条件筛选（价格、品牌、分类）
- 需要复杂排序（销量、价格、评分）

---

### 问题定位

```
┌─────────────────────────────────────────────────────────────┐
│                    问题定位过程                                │
└─────────────────────────────────────────────────────────────┘

步骤 1：查看慢查询日志
  ↓
  发现慢查询 SQL：
    SELECT * FROM products 
    WHERE title LIKE '%手机%'
      AND price BETWEEN 1000 AND 5000
      AND status = 1
    ORDER BY sales DESC
    LIMIT 20;
  
  执行时间：3 秒

步骤 2：分析执行计划
  ↓
  EXPLAIN ...
  
  结果：
    type: ALL（全表扫描）
    key: NULL（没有使用索引）
    rows: 1000000000（扫描 10 亿行）
    Extra: Using where; Using filesort
  
  问题：
    ❌ LIKE '%手机%'（% 在开头，索引失效）
    ❌ 全表扫描
    ❌ 需要排序

步骤 3：问题根源
  ↓
  问题：LIKE '%手机%' 无法使用索引
    - % 在开头，MySQL 不知道从哪里开始查找
    - 只能全表扫描
    - 扫描 10 亿行数据
```

---

### 解决方案

```
┌─────────────────────────────────────────────────────────────┐
│                    解决方案执行过程                            │
└─────────────────────────────────────────────────────────────┘

方案 1：使用全文索引（MySQL）
  ↓
  实现：
    -- 添加全文索引
    ALTER TABLE products 
    ADD FULLTEXT INDEX idx_title (title);
    
    -- 使用全文搜索
    SELECT * FROM products 
    WHERE MATCH(title) AGAINST('手机' IN NATURAL LANGUAGE MODE)
      AND price BETWEEN 1000 AND 5000
      AND status = 1
    ORDER BY sales DESC
    LIMIT 20;
  
  优点：
    ✅ 支持中文分词
    ✅ 性能比 LIKE 快
  
  缺点：
    ⚠️ 功能有限（不支持复杂搜索）
    ⚠️ 中文分词效果一般

方案 2：使用 Elasticsearch（推荐）
  ↓
  实现：
    1. 数据同步
       MySQL → Binlog → Canal → Kafka → Elasticsearch
    
    2. 搜索查询
       // Java 代码
       SearchRequest request = new SearchRequest("products");
       SearchSourceBuilder builder = new SearchSourceBuilder();
       
       // 搜索条件
       builder.query(QueryBuilders.boolQuery()
         .must(QueryBuilders.matchQuery("title", "手机"))
         .filter(QueryBuilders.rangeQuery("price").gte(1000).lte(5000))
         .filter(QueryBuilders.termQuery("status", 1))
       );
       
       // 排序
       builder.sort("sales", SortOrder.DESC);
       
       // 分页
       builder.from(0).size(20);
       
       request.source(builder);
       SearchResponse response = client.search(request);
  
  优点：
    ✅ 搜索速度快（0.1 秒）
    ✅ 支持复杂搜索（分词、高亮、聚合）
    ✅ 支持中文分词（IK 分词器）
    ✅ 支持拼音搜索
    ✅ 支持同义词
  
  结果：
    ✅ 响应时间：从 3 秒降到 0.1 秒
    ✅ 性能提升：30 倍
    ✅ 搜索准确率：提升 50%

方案 3：搜索优化
  ↓
  优化 1：搜索建议（自动补全）
    用户输入"手"
      ↓
    Elasticsearch 返回建议：
      - 手机
      - 手表
      - 手环
  
  优化 2：搜索纠错
    用户输入"shouji"（拼音）
      ↓
    Elasticsearch 自动纠正：
      - 手机
  
  优化 3：热门搜索
    缓存热门搜索词
    用户搜索热门词 → 直接从缓存返回
  
  优化 4：搜索日志分析
    分析用户搜索日志
    优化搜索算法
    提升搜索准确率
```

---

## 案例 6：出行平台 - 高并发写入优化

### 背景

**场景**：出行/外卖平台的订单抢单

**典型问题**：
- 订单状态更新延迟（3-5 秒）
- 多个司机/骑手同时抢单导致冲突
- 大量锁等待
- 用户等待时间长

**技术挑战**：
- 高并发写入（每秒数十万次）
- 需要保证数据一致性（一个订单只能被一个人接）
- 需要快速响应

---

### 问题定位

```
┌─────────────────────────────────────────────────────────────┐
│                    问题定位过程                                │
└─────────────────────────────────────────────────────────────┘

步骤 1：查看慢查询日志
  ↓
  发现慢查询 SQL：
    UPDATE orders 
    SET status = 'accepted', driver_id = 789, update_time = NOW()
    WHERE id = 123456 
      AND status = 'pending';
  
  执行时间：5 秒
  执行次数：每秒 10 万次

步骤 2：分析执行计划
  ↓
  EXPLAIN UPDATE ...
  
  结果：
    type: const（使用主键索引）
    key: PRIMARY
    rows: 1（只更新 1 行）
  
  疑问：
    ✅ 使用了主键索引
    ✅ 只更新 1 行
    ❓ 为什么还慢？

步骤 3：检查锁等待
  ↓
  SHOW ENGINE INNODB STATUS;
  
  结果：
    发现大量锁等待：
      - 事务 1：正在更新订单 123456
      - 事务 2：等待更新订单 123456
      - 事务 3：等待更新订单 123456
      ...
  
  问题发现：
    ❌ 大量事务等待锁
    ❌ 锁等待时间长

步骤 4：问题根源
  ↓
  问题：乐观锁冲突
    场景：
      1. 用户下单（订单状态：pending）
      2. 司机 A 接单（更新订单状态：accepted）
      3. 司机 B 也接单（更新订单状态：accepted）
      4. 司机 C 也接单（更新订单状态：accepted）
    
    冲突：
      - 3 个司机同时接单
      - 只有 1 个司机能成功
      - 其他 2 个司机需要重试
      - 重试导致锁等待
```

---

### 解决方案

```
┌─────────────────────────────────────────────────────────────┐
│                    解决方案执行过程                            │
└─────────────────────────────────────────────────────────────┘

方案 1：使用分布式锁（Redis）
  ↓
  实现：
    // 司机接单
    public boolean acceptOrder(Long orderId, Long driverId) {
      // 1. 尝试获取分布式锁
      String lockKey = "order:lock:" + orderId;
      boolean locked = redis.setnx(lockKey, driverId, 10);  // 10 秒过期
      
      if (!locked) {
        return false;  // 获取锁失败，订单已被其他司机接单
      }
      
      try {
        // 2. 更新订单状态
        int rows = orderMapper.updateStatus(orderId, "accepted", driverId);
        
        if (rows == 0) {
          return false;  // 更新失败，订单已被其他司机接单
        }
        
        // 3. 通知用户
        notifyUser(orderId, driverId);
        
        return true;
      } finally {
        // 4. 释放锁
        redis.del(lockKey);
      }
    }
  
  优点：
    ✅ 避免锁冲突
    ✅ 性能高
    ✅ 响应时间：从 5 秒降到 0.1 秒

方案 2：使用消息队列（Kafka）
  ↓
  实现：
    1. 用户下单
       用户 → 订单服务 → Kafka（订单消息）
    
    2. 司机抢单
       司机 → 抢单服务 → Kafka（抢单消息）
    
    3. 订单分配
       Kafka → 订单分配服务 → 分配订单给司机
       
       订单分配逻辑：
         - 从 Kafka 读取抢单消息
         - 按时间顺序处理
         - 第一个抢单的司机获得订单
         - 其他司机抢单失败
    
    4. 通知司机
       订单分配服务 → Kafka → 通知服务 → 司机
  
  优点：
    ✅ 解耦（订单服务和抢单服务解耦）
    ✅ 异步处理（不阻塞用户下单）
    ✅ 高并发（Kafka 支持高并发）
    ✅ 可靠性（消息不丢失）

方案 3：使用数据库乐观锁 + 版本号
  ↓
  实现：
    -- 添加版本号字段
    ALTER TABLE orders ADD COLUMN version INT DEFAULT 0;
    
    -- 更新订单状态（带版本号）
    UPDATE orders 
    SET status = 'accepted', 
        driver_id = 789, 
        version = version + 1,
        update_time = NOW()
    WHERE id = 123456 
      AND status = 'pending'
      AND version = 0;  -- 版本号匹配才能更新
    
    -- Java 代码
    public boolean acceptOrder(Long orderId, Long driverId) {
      // 1. 查询订单
      Order order = orderMapper.selectById(orderId);
      
      if (!"pending".equals(order.getStatus())) {
        return false;  // 订单已被接单
      }
      
      // 2. 更新订单状态（带版本号）
      int rows = orderMapper.updateWithVersion(
        orderId, "accepted", driverId, order.getVersion()
      );
      
      if (rows == 0) {
        return false;  // 更新失败，订单已被其他司机接单
      }
      
      return true;
    }
  
  优点：
    ✅ 避免锁冲突
    ✅ 实现简单
  
  缺点：
    ⚠️ 需要重试（更新失败需要重试）
```

---

## 大厂 SQL 优化经验总结

### 1. 监控和告警

```
监控指标：
  ✅ 慢查询数量
  ✅ 慢查询 SQL
  ✅ 数据库 CPU
  ✅ 数据库内存
  ✅ 数据库磁盘 IO
  ✅ 数据库连接数
  ✅ 接口响应时间

告警规则：
  ✅ 慢查询数量 > 1000/分钟
  ✅ 数据库 CPU > 80%
  ✅ 接口响应时间 > 1 秒
  ✅ 数据库连接数 > 80%

告警方式：
  ✅ 短信
  ✅ 电话
  ✅ 钉钉/企业微信
  ✅ 邮件
```

---

### 2. 应急预案

```
预案 1：紧急扩容
  - 增加数据库实例
  - 增加服务器
  - 增加缓存

预案 2：降级
  - 关闭非核心功能
  - 限流
  - 熔断

预案 3：切换
  - 主从切换
  - 读写分离
  - 切换到备用数据库

预案 4：回滚
  - 回滚代码
  - 回滚数据库
  - 回滚配置
```

---

### 3. 优化技巧

```
技巧 1：索引优化
  ✅ 给常用查询字段添加索引
  ✅ 使用联合索引
  ✅ 避免索引失效
  ✅ 定期分析索引使用情况

技巧 2：SQL 优化
  ✅ 避免 SELECT *
  ✅ 避免子查询（改用 JOIN）
  ✅ 避免 LIKE '%xxx%'
  ✅ 使用 LIMIT 限制返回数据量

技巧 3：缓存优化
  ✅ 使用 Redis 缓存热点数据
  ✅ 使用 CDN 缓存静态资源
  ✅ 使用本地缓存（Guava Cache）

技巧 4：架构优化
  ✅ 读写分离
  ✅ 分库分表
  ✅ 使用消息队列
  ✅ 使用 Elasticsearch（搜索）
  ✅ 使用 MongoDB（位置数据）

技巧 5：数据优化
  ✅ 冷热数据分离
  ✅ 历史数据归档
  ✅ 定期清理无用数据
  ✅ 数据压缩
```

---

### 4. 优化流程

```
┌─────────────────────────────────────────────────────────────┐
│                  大厂 SQL 优化标准流程                         │
└─────────────────────────────────────────────────────────────┘

步骤 1：发现问题
  ↓
  - 监控告警
  - 用户反馈
  - 慢查询日志

步骤 2：紧急响应
  ↓
  - 成立应急小组
  - 评估影响范围
  - 制定应急方案

步骤 3：问题定位
  ↓
  - 查看慢查询日志
  - 分析执行计划（EXPLAIN）
  - 检查索引
  - 检查表结构
  - 检查锁等待

步骤 4：紧急处理
  ↓
  - 添加索引
  - 优化 SQL
  - 增加缓存
  - 扩容
  - 降级

步骤 5：验证效果
  ↓
  - 测试性能
  - 监控指标
  - 用户反馈

步骤 6：长期优化
  ↓
  - 架构优化
  - 分库分表
  - 冷热数据分离
  - 数据归档

步骤 7：复盘总结
  ↓
  - 问题原因
  - 解决方案
  - 经验教训
  - 改进措施
```

---

## 一句话总结

**大厂 SQL 优化的核心：**

1. **提前预防**：做好监控、告警、容量规划
2. **快速响应**：发现问题立即响应，成立应急小组
3. **精准定位**：使用 EXPLAIN、慢查询日志快速定位问题
4. **分层处理**：紧急方案（立即生效）+ 长期方案（彻底解决）
5. **持续优化**：架构优化、分库分表、缓存、消息队列
6. **复盘总结**：每次问题都要复盘，避免再次发生

**记住：大厂的 SQL 优化不是单纯的技术问题，而是系统工程，需要监控、告警、应急预案、架构优化、团队协作等多方面配合！**

