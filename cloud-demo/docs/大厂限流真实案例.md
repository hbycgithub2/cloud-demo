# 大厂限流真实案例

## 一、淘宝双11限流案例

### 场景：双11秒杀

**问题：**
- 平时：每秒 10 万次请求
- 双11零点：每秒 1000 万次请求（暴增 100 倍）
- 服务器扛不住，会崩溃

---

### 淘宝的限流方案（多层限流）

```
┌─────────────────────────────────────────────────────────────────┐
│  第 1 层：CDN 限流（最外层）                                     │
│                                                                 │
│  规则：每个 IP 每秒最多 100 个请求                               │
│                                                                 │
│  1000 万次请求 → CDN 限流 → 100 万次请求通过                     │
│                                                                 │
│  拦截：900 万次请求（恶意刷单、爬虫）                            │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ 100 万次请求
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│  第 2 层：Gateway 限流（接口级别）                               │
│                                                                 │
│  规则：                                                          │
│    - 秒杀接口：每秒最多 10 万次请求                              │
│    - 普通接口：每秒最多 50 万次请求                              │
│                                                                 │
│  100 万次请求 → Gateway 限流 → 60 万次请求通过                   │
│                                                                 │
│  拦截：40 万次请求（超过接口限制）                               │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ 60 万次请求
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│  第 3 层：服务限流（用户级别）                                   │
│                                                                 │
│  规则：每个用户每秒最多 10 个请求                                │
│                                                                 │
│  60 万次请求 → 服务限流 → 50 万次请求通过                        │
│                                                                 │
│  拦截：10 万次请求（单个用户恶意刷接口）                         │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ 50 万次请求
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│  第 4 层：数据库限流（资源级别）                                 │
│                                                                 │
│  规则：数据库连接池最多 1000 个连接                              │
│                                                                 │
│  50 万次请求 → 数据库限流 → 排队处理                             │
│                                                                 │
│  保护：数据库不会被打垮                                          │
└─────────────────────────────────────────────────────────────────┘
```

**结果：**
- 原始请求：1000 万次/秒
- 最终处理：50 万次/秒
- 拦截率：95%
- 服务器稳定运行 ✅

---

### 淘宝的限流策略

#### 策略 1：分级限流

```
VIP 用户（会员）：
  - 限流：每秒 100 个请求
  - 优先级：高
  - 排队：优先处理

普通用户：
  - 限流：每秒 10 个请求
  - 优先级：中
  - 排队：正常处理

游客（未登录）：
  - 限流：每秒 1 个请求
  - 优先级：低
  - 排队：最后处理
```

**人话：**
- VIP 用户可以刷 100 次，普通用户只能刷 10 次
- 就像机场的头等舱和经济舱，头等舱优先登机

---

#### 策略 2：动态限流

```
平时（流量低）：
  - 限流：每秒 10 万次请求
  - 服务器：100 台

双11（流量高）：
  - 限流：每秒 100 万次请求
  - 服务器：1000 台（自动扩容）

凌晨（流量低）：
  - 限流：每秒 1 万次请求
  - 服务器：10 台（自动缩容）
```

**人话：**
- 根据流量自动调整限流规则
- 就像餐厅，中午人多多开几个窗口，晚上人少关掉几个窗口

---

#### 策略 3：熔断降级

```
正常情况：
  - 秒杀接口：正常访问
  - 商品详情：正常访问
  - 推荐系统：正常访问

流量暴增（服务器扛不住）：
  - 秒杀接口：正常访问（核心功能，必须保证）
  - 商品详情：正常访问（核心功能，必须保证）
  - 推荐系统：关闭（非核心功能，可以牺牲）
```

**人话：**
- 保证核心功能，牺牲非核心功能
- 就像飞机遇到紧急情况，先保证飞行安全，娱乐系统可以关掉

---

## 二、京东618限流案例

### 场景：618抢购

**问题：**
- 某款手机限量 1000 台
- 100 万人同时抢购
- 只有 1000 人能抢到

---

### 京东的限流方案（令牌桶 + 队列）

```
┌─────────────────────────────────────────────────────────────────┐
│  第 1 步：令牌桶限流                                             │
│                                                                 │
│  规则：                                                          │
│    - 桶里有 1000 个令牌（对应 1000 台手机）                      │
│    - 每个用户抢购需要 1 个令牌                                   │
│                                                                 │
│  100 万人抢购：                                                  │
│    - 前 1000 人拿到令牌 ✅                                       │
│    - 后 999000 人没拿到令牌 ❌                                   │
│                                                                 │
│  结果：只有 1000 人进入下单流程                                  │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ 1000 人
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│  第 2 步：消息队列排队                                           │
│                                                                 │
│  1000 人进入消息队列：                                           │
│    - 用户 1 → 队列位置 1                                         │
│    - 用户 2 → 队列位置 2                                         │
│    ...                                                          │
│    - 用户 1000 → 队列位置 1000                                   │
│                                                                 │
│  按顺序处理：                                                    │
│    - 每秒处理 100 个订单                                         │
│    - 1000 个订单需要 10 秒处理完                                 │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ 按顺序处理
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│  第 3 步：创建订单                                               │
│                                                                 │
│  用户 1：                                                        │
│    - 检查库存：1000 - 1 = 999 ✅                                │
│    - 创建订单 ✅                                                 │
│    - 扣减库存：999 台                                            │
│                                                                 │
│  用户 2：                                                        │
│    - 检查库存：999 - 1 = 998 ✅                                 │
│    - 创建订单 ✅                                                 │
│    - 扣减库存：998 台                                            │
│                                                                 │
│  ...                                                            │
│                                                                 │
│  用户 1000：                                                     │
│    - 检查库存：1 - 1 = 0 ✅                                     │
│    - 创建订单 ✅                                                 │
│    - 扣减库存：0 台                                              │
│                                                                 │
│  用户 1001（如果有）：                                           │
│    - 检查库存：0 台 ❌                                           │
│    - 创建订单失败：库存不足                                      │
└─────────────────────────────────────────────────────────────────┘
```

**结果：**
- 100 万人抢购
- 只有 1000 人拿到令牌
- 1000 人成功下单
- 999000 人失败（显示"已售罄"）
- 服务器稳定运行 ✅

---

### 京东的优化：预扣库存

```
传统方案（有问题）：
  1. 用户点击"立即购买"
  2. 检查库存
  3. 创建订单
  4. 扣减库存
  
  问题：
    - 步骤 2 和步骤 4 之间有时间差
    - 可能导致超卖（库存只有 1 台，但 2 个人同时下单）

京东方案（预扣库存）：
  1. 用户点击"立即购买"
  2. 立即扣减库存（预扣）
  3. 创建订单
  4. 如果订单创建失败，恢复库存
  
  优点：
    - 不会超卖
    - 库存准确
```

---

## 三、抖音直播限流案例

### 场景：明星直播

**问题：**
- 某明星直播
- 1000 万人同时观看
- 弹幕、点赞、送礼物等请求暴增

---

### 抖音的限流方案（分布式限流）

```
┌─────────────────────────────────────────────────────────────────┐
│  架构：                                                          │
│                                                                 │
│  1000 万用户                                                     │
│    ↓                                                            │
│  100 台 Gateway 服务器（每台处理 10 万用户）                     │
│    ↓                                                            │
│  Redis 集群（分布式限流）                                        │
│    ↓                                                            │
│  1000 台直播服务器                                               │
└─────────────────────────────────────────────────────────────────┘
```

#### 限流规则

```
1. 弹幕限流：
   - 每个用户每秒最多发 1 条弹幕
   - 防止刷屏

2. 点赞限流：
   - 每个用户每秒最多点赞 10 次
   - 防止恶意刷赞

3. 送礼物限流：
   - 每个用户每秒最多送 5 个礼物
   - 防止误操作

4. 进入直播间限流：
   - 每秒最多 10 万人进入
   - 防止服务器崩溃
```

#### 分布式限流实现

```java
// 使用 Redis 实现分布式限流
@Component
public class DistributedRateLimiter {
    
    @Autowired
    private StringRedisTemplate redisTemplate;
    
    /**
     * 限流检查
     * @param userId 用户 ID
     * @param action 操作类型（弹幕、点赞、送礼物）
     * @param maxCount 最大次数
     * @return true=通过，false=拒绝
     */
    public boolean checkRateLimit(Long userId, String action, int maxCount) {
        // 1. 构造 Redis key
        String currentSecond = String.valueOf(System.currentTimeMillis() / 1000);
        String key = "rate_limit:" + action + ":" + userId + ":" + currentSecond;
        
        // 2. 计数器 +1
        Long count = redisTemplate.opsForValue().increment(key);
        
        // 3. 设置过期时间（2 秒后自动删除）
        if (count == 1) {
            redisTemplate.expire(key, 2, TimeUnit.SECONDS);
        }
        
        // 4. 检查是否超过限制
        return count <= maxCount;
    }
}

// 使用示例
@RestController
public class LiveController {
    
    @Autowired
    private DistributedRateLimiter rateLimiter;
    
    // 发送弹幕
    @PostMapping("/live/danmu")
    public Result sendDanmu(@RequestBody DanmuRequest request) {
        // 限流检查：每个用户每秒最多发 1 条弹幕
        if (!rateLimiter.checkRateLimit(request.getUserId(), "danmu", 1)) {
            return Result.error("发送太快了，请稍后再试");
        }
        
        // 发送弹幕
        liveService.sendDanmu(request);
        return Result.success();
    }
    
    // 点赞
    @PostMapping("/live/like")
    public Result like(@RequestBody LikeRequest request) {
        // 限流检查：每个用户每秒最多点赞 10 次
        if (!rateLimiter.checkRateLimit(request.getUserId(), "like", 10)) {
            return Result.error("点赞太快了，请稍后再试");
        }
        
        // 点赞
        liveService.like(request);
        return Result.success();
    }
}
```

---

## 四、微信红包限流案例

### 场景：春节抢红包

**问题：**
- 除夕夜 12 点
- 全国人民同时抢红包
- 每秒 100 万次请求

---

### 微信的限流方案（令牌桶 + 预分配）

```
┌─────────────────────────────────────────────────────────────────┐
│  第 1 步：预分配红包                                             │
│                                                                 │
│  张三发了一个红包：                                              │
│    - 总金额：100 元                                              │
│    - 红包个数：10 个                                             │
│                                                                 │
│  微信提前分配好：                                                │
│    - 红包 1：15 元                                               │
│    - 红包 2：8 元                                                │
│    - 红包 3：12 元                                               │
│    ...                                                          │
│    - 红包 10：9 元                                               │
│                                                                 │
│  存入 Redis：                                                    │
│    key: red_packet:123456                                       │
│    value: [15, 8, 12, ..., 9]                                   │
└─────────────────────────────────────────────────────────────────┘
                            │
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│  第 2 步：抢红包（令牌桶限流）                                   │
│                                                                 │
│  100 人同时抢红包：                                              │
│                                                                 │
│  用户 1：                                                        │
│    - 从 Redis 弹出第 1 个红包：15 元 ✅                          │
│    - 抢到了！                                                    │
│                                                                 │
│  用户 2：                                                        │
│    - 从 Redis 弹出第 2 个红包：8 元 ✅                           │
│    - 抢到了！                                                    │
│                                                                 │
│  ...                                                            │
│                                                                 │
│  用户 10：                                                       │
│    - 从 Redis 弹出第 10 个红包：9 元 ✅                          │
│    - 抢到了！                                                    │
│                                                                 │
│  用户 11：                                                       │
│    - 从 Redis 弹出红包：空 ❌                                    │
│    - 红包已被抢完                                                │
│                                                                 │
│  ...                                                            │
│                                                                 │
│  用户 100：                                                      │
│    - 从 Redis 弹出红包：空 ❌                                    │
│    - 红包已被抢完                                                │
└─────────────────────────────────────────────────────────────────┘
```

**优点：**
- ✅ 不需要计算红包金额（提前分配好了）
- ✅ 不会超发（Redis 保证原子性）
- ✅ 性能极高（只需要从 Redis 弹出数据）

---

## 五、大厂限流最佳实践总结

### 1. 多层限流（推荐）✅

```
第 1 层：CDN 限流
  - 拦截恶意请求、爬虫
  - 拦截率：90%

第 2 层：Gateway 限流
  - 接口级别限流
  - 拦截率：5%

第 3 层：服务限流
  - 用户级别限流
  - 拦截率：3%

第 4 层：数据库限流
  - 资源级别限流
  - 保护数据库

总拦截率：98%
```

---

### 2. 分级限流（推荐）✅

```
VIP 用户：
  - 限流：宽松
  - 优先级：高
  - 体验：好

普通用户：
  - 限流：正常
  - 优先级：中
  - 体验：一般

游客：
  - 限流：严格
  - 优先级：低
  - 体验：差
```

---

### 3. 动态限流（推荐）✅

```
根据流量自动调整：
  - 流量高 → 限流宽松 + 自动扩容
  - 流量低 → 限流严格 + 自动缩容
```

---

### 4. 熔断降级（推荐）✅

```
服务器扛不住时：
  - 核心功能：保证（秒杀、下单、支付）
  - 非核心功能：关闭（推荐、评论、分享）
```

---

### 5. 令牌桶 + 队列（推荐）✅

```
令牌桶：
  - 快速拦截大部分请求
  - 只有少数请求拿到令牌

消息队列：
  - 拿到令牌的请求进入队列
  - 按顺序处理
  - 防止数据库被打垮
```

---

## 六、限流配置建议

### 接口限流配置

```yaml
# 秒杀接口（核心）
/seckill/**:
  rate: 10000/s      # 每秒 1 万次
  burst: 20000       # 突发流量 2 万次
  priority: high     # 高优先级

# 下单接口（核心）
/order/create:
  rate: 5000/s       # 每秒 5000 次
  burst: 10000       # 突发流量 1 万次
  priority: high     # 高优先级

# 查询接口（普通）
/product/list:
  rate: 50000/s      # 每秒 5 万次
  burst: 100000      # 突发流量 10 万次
  priority: medium   # 中优先级

# 推荐接口（非核心）
/recommend/**:
  rate: 10000/s      # 每秒 1 万次
  burst: 20000       # 突发流量 2 万次
  priority: low      # 低优先级
  fallback: true     # 可以降级
```

---

### 用户限流配置

```yaml
# VIP 用户
vip:
  rate: 100/s        # 每秒 100 次
  burst: 200         # 突发流量 200 次

# 普通用户
normal:
  rate: 10/s         # 每秒 10 次
  burst: 20          # 突发流量 20 次

# 游客
guest:
  rate: 1/s          # 每秒 1 次
  burst: 2           # 突发流量 2 次
```

---

## 七、总结

### 大厂限流的核心思想

```
1. 多层限流
   - CDN → Gateway → 服务 → 数据库
   - 层层拦截，保护核心

2. 分级限流
   - VIP > 普通用户 > 游客
   - 保证重要用户体验

3. 动态限流
   - 根据流量自动调整
   - 自动扩容/缩容

4. 熔断降级
   - 保证核心功能
   - 牺牲非核心功能

5. 令牌桶 + 队列
   - 快速拦截
   - 按顺序处理
```

### 真实案例对比

| 场景 | 原始请求 | 最终处理 | 拦截率 | 结果 |
|------|---------|---------|--------|------|
| 淘宝双11 | 1000 万/秒 | 50 万/秒 | 95% | 稳定 ✅ |
| 京东618 | 100 万人抢购 | 1000 人成功 | 99.9% | 稳定 ✅ |
| 抖音直播 | 1000 万人观看 | 分布式处理 | 动态 | 稳定 ✅ |
| 微信红包 | 100 万/秒 | 预分配 | 99% | 稳定 ✅ |

### 一句话总结

**大厂限流的核心是"多层限流 + 分级限流 + 动态限流 + 熔断降级"。淘宝双11用多层限流拦截 95% 的请求，京东618用令牌桶 + 队列保证 1000 人成功抢购，抖音直播用分布式限流处理 1000 万人观看，微信红包用预分配 + Redis 保证性能。关键是：保证核心功能，牺牲非核心功能，让服务器稳定运行！**
